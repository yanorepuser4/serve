---
llama-2-13b:
  int8:
    benchmark_engine: "ab"
    url: https://torchserve.s3.amazonaws.com/mar_files/llama2-7b-chat/llama-2-13b-int8-tp.mar
    workers:
      - 4
    batch_delay: 100
    batch_size:
      - 1
    input: "./examples/large_models/gpt_fast/request.json"
    requests: 1000
    concurrency: 4
    backend_profiling: False
    exec_env: "local"
    processors:
      - "gpus": "all"
    stream: "false"
  int4:
    benchmark_engine: "ab"
    url: https://torchserve.s3.amazonaws.com/mar_files/llama2-7b-chat/llama-2-13b-int4-tp.mar
    workers:
      - 4
    batch_delay: 100
    batch_size:
      - 1
    input: "./examples/large_models/gpt_fast/request.json"
    requests: 1000
    concurrency: 4
    backend_profiling: False
    exec_env: "local"
    processors:
      - "gpus": "all"
    stream: "false"

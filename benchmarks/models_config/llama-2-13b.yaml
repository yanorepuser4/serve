---
llama-2-13b:
  eager-mode:
    benchmark_backend: "locust"
    url: https://torchserve.s3.amazonaws.com/mar_files/llama2-7b-chat/llama-2-13b.mar
    workers:
      - 4
    batch_delay: 100
    batch_size:
      - 1
    input: "./examples/large_models/gpt_fast/request.json"
    requests: 1000
    concurrency: 4
    backend_profiling: False
    exec_env: "local"
    processors:
      - "gpus": "all"
    stream: "false"
